{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE 534 HW: Reinforcement Learning\n",
    "    v1, Designed for IE 534/CS 547 Deep Learning, Fall 2019 at UIUC\n",
    "\n",
    "In this assignment, we will experiment with the (deep) reinforcement learning algorithms covered in the lecture. In particular, you will implement variants of the popular `DQN` (Deep Q-Network) (1) and `A2C` (Advantage Actor-Critic) (2) algorithms (by the same first author! orz), and test your implementation on both a small example (CartPole problem) and an Atari game (Breakout game). We focus on model-free algorithms rather than model-based ones, because neural nets are easier applicable and more popular nowadays in the model-free setting. (When the system dynamic is known or can be easily inferred, model-based can sometimes do better.)\n",
    "\n",
    "The assignment breaks into **three parts**:\n",
    "\n",
    "- **In Part I** (50 pts), you basically need to follow the instructions in this notebook to do a little bit of coding. We'll be able to see if your code trains by testing against the CartPole environment provided by the OpenAI gym package. We'll generate some plots that are required for grading.\n",
    "\n",
    "- **In Part II** (40 pts), you'll copy your code onto Blue Waters (or actually any good server..), and run a much larger-scale experiment with the Breakout game. Hopefully, you can teach the computer to play Breakout in less than half a day! Share your final game score in this notebook. **<font color=red>This part will take at least a day. Please start early!!</font>**\n",
    "\n",
    "- **In Part III** (10 pts), you'll be asked to think about a few questions. These questions are mostly open-ended. Please write down your thoughts on them.\n",
    "\n",
    "Finally, after you finished everything in this notebook **<font color=red>(code snippets C1-C5, plots P1-P5, question answers Q1-Q5)</font>**, please save the notebook, and export to a PDF (or an HTML file), and submit:\n",
    "    \n",
    "1. the **.ipynb notebook and exported .pdf/.html file**, PDF is preferred (I usually do File -> Print Preview -> use Chrome's Save as PDF);\n",
    "\n",
    "2. your code (**Algo.py, Model.py files**);\n",
    "\n",
    "3. job artifacts (**.log files** only, pytorch models and images not required)\n",
    "\n",
    "to Compass 2g for grading.\n",
    "\n",
    "**PS: Remember to save your notebook occasionally as you work through it!**\n",
    "\n",
    "#### References\n",
    "\n",
    "- (1) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. Nature, 518(7540), p.529.\n",
    "- (2) Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement learning. In International conference on machine learning (pp. 1928-1937).\n",
    "- (3) A useful tutorial: https://spinningup.openai.com/en/latest/\n",
    "- (4) *Useful code references*: https://github.com/deepmind/bsuite; https://github.com/openai/baselines; https://github.com/astooke/rlpyt;\n",
    "\n",
    "***\n",
    "First of all, **enter your NetID here** in the cell below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your NetID: tsmurra2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: DQN and A2C on CartPole\n",
    "***\n",
    "This part is designed to run on your own local laptop/PC.\n",
    "\n",
    "Before you start, there are some python dependencies: `pytorch, gym, numpy, multiprocessing, matplotlib`. Please install them correctly. You can install `pytorch` following instruction here https://pytorch.org/get-started/locally/. The code is compatible with PyTorch 0.4.x ~ 1.x. PyTorch 1.1 with cuda 10.0 worked for me (`conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch`).\n",
    "\n",
    "Please <font color=red>**always**</font> run the code cell below each time you open this notebook, to make sure `gym` is installed and to enable `autoreload` which **allows code changes to be effective immediately**. So if you changed `Algo.py` or `Model.py` but the test codes are not reflecting your changes, restart the notebook kernel and run this cell!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install gym\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "# install openai gym\n",
    "%pip install gym\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Code Structure\n",
    "\n",
    "The code is structured in 5 python files:\n",
    "\n",
    "- `Main.py`: contains the main entry point and training loop\n",
    "- `Model.py`: constructs the torch neural network modules\n",
    "- `Env.py`: contains the environment simulations interface, based on openai gym\n",
    "- `Algo.py`: implements the DQN and A2C algorithms\n",
    "- `Replay.py`: implements the experience replay buffer for DQN\n",
    "- `Draw.py`: saves some game snapshots to jpeg files\n",
    "\n",
    "Some parts of the code `Model.py` and `Algo.py` are left blank for you to complete. You are not required to modify the other parts (unless, of course, you want to boost the performance!). This is kind of a minimalist implementation, and might be different from the other code on the internet in details. You're welcomed to improve it,  after you've finished all the required things of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 OpenAI gym and CartPole environment\n",
    "OpenAI developed python package `gym` a while ago to facilitate RL research. `gym` provides a common interface between the program and the environments. For instance, the code cell below will create the CartPole environment. A window will show up when you run the code. The goal is to keep adjusting the cart so that the pole stays in its upright position.\n",
    "\n",
    "A demo video from OpenAI:\n",
    "<video width=\"320\" controls src=\"http://s3-us-west-2.amazonaws.com/rl-gym-doc/cartpole-no-reset.mp4\" />\n",
    "\n",
    "`gym` also provides interface to Atari games. However, installing package `atari-py` is not easy on Windows/Mac, so we won't demonstrate it here. More info: http://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "    if done: break\n",
    "    time.sleep(0.15)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Deep Q Learning\n",
    "\n",
    "A little recap on DQN. We learned from lecture that Q-Learning is a model-free reinforcement learning algorithm. It falls into the off-policy type algorithm since it can utilize past experiences stored in a buffer. It also falls into the (approximate) dynamic programming type algorithm, since it tries to learn an optimal state-action value function using time difference (TD) errors. Q Learning is particularly interesting because it exploits the optimality structure in MDP. It's related to the Hamilton–Jacobi–Bellman equation in classical control.\n",
    "\n",
    "For MDP\n",
    "$$\n",
    "M = (S,A,P,r,\\gamma)\n",
    "$$\n",
    "where $S$ is the state space, $A$ is the action space, $P$ is the transition dynamic, $r(s,a)$ is a reward function $S\\times A \\mapsto R$, and $\\gamma$ is the discount factor.\n",
    "\n",
    "The tabular case (when $S,A$ are finite), Q-Learning does the following value iteration update repeatedly when collecting experience $(s_t, a_t, r_t)$ ($\\eta$ is the learning rate):\n",
    "$$\n",
    "Q^{new}(s_t, a_t) \\leftarrow Q^{old}(s_t, a_t) + \\eta \\left( r_t + \\gamma \\max_{a'\\in A} Q^{old}(s_t, a') - Q^{old}(s_t, a_t) \\right) .\n",
    "$$\n",
    "\n",
    "With function approximation, meaning model $Q(s,a)$ with a function $Q_{\\theta}(s,a)$ parameterized by $\\theta$, we arrive at the Fitted Q Iteration (FQI) algorithm, or better known as Deep Q Learning if the function class is neural networks. Q-Learning with neural network as function approximator was known long ago, but it was only recently (year 2013) that DeepMind made this algorithm actually work on Atari games. Deep Q Learning iteratively optimize the following objective:\n",
    "$$\n",
    "\\theta_{new} \\leftarrow \\arg\\min_{\\theta} \\mathbb{E}_{(s,a,r,s')\\sim D} \\left( r + \\gamma \\max_{a'\\in A} Q_{\\theta_{old}}(s', a') - Q_{\\theta}(s, a) \\right)^2  .\n",
    "$$\n",
    "\n",
    "Therefore, with a batch of $\\{(s^i,a^i,r^i,s'^i)\\}_{i=1}^N$ sampled from the replay buffer, we can build a loss function $L$ in pytorch:\n",
    "$$\n",
    "L(\\theta) = \\frac1N \\sum_{i=1}^N \\left( r^i + \\gamma \\max_{a'\\in A} Q_{\\theta_{old}}(s'^i, a') - Q_{\\theta}(s^i, a^i) \\right)^2\n",
    ",\n",
    "$$\n",
    "and run the usual gradient descent on $\\theta$ with a pytorch optimizer.\n",
    "\n",
    "\n",
    "#### Exploration\n",
    "Exploration, as the word suggests, refers to explore novel unvisited states in RL. The FQI (or DQN) needs an exploratory datasets to work well. The common way to produce exploratory dataset is through randomization, such as the $\\epsilon$-greedy exploration strategy we will implement in this assignment.\n",
    "- $\\epsilon$-greedy exploration:\n",
    "\n",
    "At training iteration $it$, the agent chooses to play\n",
    "$$\n",
    "a = \\begin{cases}\n",
    "\\arg\\max_a Q_{\\theta}(s, a)      &  \\text{ with probability $1 - \\epsilon_{it}$ },  \\\\\n",
    "\\text{a random action $a \\in A$} &  \\text{ with probability $\\epsilon_{it}$ }.  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "And $\\epsilon_{it}$ is annealed, for example, linearly from $1$ to $0.01$ as training progresses until iteration $it_{\\text{decay}}$:\n",
    "$$\n",
    "\\epsilon_{it} = \\max\\Big\\{ 0.01, 1 + (0.01-1)\\frac{it}{it_{\\text{decay}}} \\Big\\}.\n",
    "$$\n",
    "\n",
    "#### Two Caveats\n",
    "1. There's an improvement on DQN called Double-DQN with the following loss $L$, which has shown to be empirically more stable than the original DQN loss described above. You may want to implement the improved one in your code:\n",
    "$$\n",
    "L(\\theta) = \\frac1N \\sum_{i=1}^N \\left( r^i + \\gamma Q_{\\theta_{old}}\\big( s'^i, \\arg\\max_{a'\\in A} Q_{\\theta}(s'^i, a' ) \\big) - Q_{\\theta}(s^i, a^i) \\right)^2\n",
    ".\n",
    "$$\n",
    "2. Huber loss (a.k.a smooth L1 loss) is commonly used to reduce the effect of extreme values:\n",
    "$$\n",
    "L(\\theta) = \\frac1N \\sum_{i=1}^N Huber\\left( r^i + \\gamma Q_{\\theta_{old}}\\big( s'^i, \\arg\\max_{a'\\in A} Q_{\\theta}(s'^i, a' ) \\big) - Q_{\\theta}(s^i, a^i) \\right)\n",
    "$$\n",
    "You can look up the pytorch document here: https://pytorch.org/docs/stable/nn.functional.html#smooth-l1-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C1 (5 pts): Complete the code for the two layered fully connected network class `TwoLayerFCNet` in file `Model.py`\n",
    "And run the cell below to test the output shape of your module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape test passed!\n"
     ]
    }
   ],
   "source": [
    "## Test code\n",
    "from Model import TwoLayerFCNet\n",
    "import torch\n",
    "net = TwoLayerFCNet(n_in=4, n_hidden=16, n_out=5)\n",
    "x = torch.randn(10, 4)\n",
    "y = net(x)\n",
    "assert y.shape == (10, 5), \"ERROR: network output has the wrong shape!\"\n",
    "print (\"Output shape test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C2 (5 pts): Complete the code for $\\epsilon$-greedy exploration strategy in function `DQN.act` in file `Algo.py'\n",
    "And run the cell below to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy test passed!\n"
     ]
    }
   ],
   "source": [
    "## Test code\n",
    "from Algo import DQN\n",
    "class Nothing: pass\n",
    "dummy = Nothing()\n",
    "dummy.eps_decay = 500000\n",
    "\n",
    "dummy.num_act_steps = 0\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 1.0 ) < 0.01, \"ERROR: compute_epsilon at t=0 should be 1 but got %f!\" % eps\n",
    "\n",
    "dummy.num_act_steps = 250000\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 0.505 ) < 0.01, \"ERROR: compute_epsilon at t=250000 should around .505 but got %f!\" % eps\n",
    "\n",
    "dummy.num_act_steps = 500000\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 0.01 ) < 0.01, \"ERROR: compute_epsilon at t=500000 should be .01 but got %f!\" % eps\n",
    "\n",
    "dummy.num_act_steps = 600000\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 0.01 ) < 0.01, \"ERROR: compute_epsilon after t=500000 should be .01 but got %f!\" % eps\n",
    "print (\"Epsilon-greedy test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C3 (10 pts): Complete the code for computing the loss function in `DQN.train` in file `Algo.py`\n",
    "And run the cell below to verify your code decreses the loss value in one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters to optimize: [('fc1.weight', torch.Size([128, 10]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([3, 128]), True), ('fc2.bias', torch.Size([3]), True)] \n",
      "\n",
      "0.22743602097034454 > 0.22402134537696838 ?\n",
      "DQN.train test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Algo import DQN\n",
    "class Nothing: pass\n",
    "dummy_obs_space, dummy_act_space = Nothing(), Nothing()\n",
    "dummy_obs_space.shape = [10]\n",
    "dummy_act_space.n = 3\n",
    "\n",
    "dqn = DQN(dummy_obs_space, dummy_act_space, batch_size=2)\n",
    "\n",
    "for t in range(3):\n",
    "    dqn.observe([np.random.randn(10).astype('float32')], [np.random.randint(3)],\n",
    "                [(np.random.randn(10).astype('float32'), np.random.rand(), False, None)])\n",
    "\n",
    "b = dqn.replay.cur_batch\n",
    "loss1 = dqn.train()\n",
    "dqn.replay.cur_batch = b\n",
    "loss2 = dqn.train()\n",
    "\n",
    "print (loss1, '>', loss2, '?')\n",
    "assert loss2 < loss1, \"DQN.train should reduce loss on the same batch\"\n",
    "\n",
    "print (\"DQN.train test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P1 (10 pts): Run DQN on CartPole and plot the learning curve (i.e. averaged episodic reward against env steps).\n",
    "Your code should be able to achieve **>150** averaged reward in 10000 iterations (20000 simulation steps) in only a few minutes. This is a good indication that the implementation is correct. It's ok that the curve is not always monotonically increasing because of randomness in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(algo='dqn', batch_size=64, checkpoint_freq=20000, discount=0.996, ent_coef=0.01, env='CartPole-v1', eps_decay=4000, frame_skip=1, frame_stack=4, load='', log='log.txt', lr=0.001, niter=10000, nproc=2, parallel_env=0, print_freq=200, replay_size=20000, save_dir='cartpole_dqn/', target_update=1000, train_freq=1, train_start=100, value_coef=0.5)\n",
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "running on device cuda\n",
      "parameters to optimize: [('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([2, 128]), True), ('fc2.bias', torch.Size([2]), True)] \n",
      "\n",
      "obses on reset: 2 x (4,) float32\n",
      "iter    200 |loss   0.01 |n_ep    16 |ep_len   22.4 |ep_rew  22.40 |raw_ep_rew  22.40 |env_step    400 |time 00:02 rem 01:43\n",
      "iter    400 |loss   0.00 |n_ep    32 |ep_len   24.9 |ep_rew  24.87 |raw_ep_rew  24.87 |env_step    800 |time 00:06 rem 02:30\n",
      "iter    600 |loss   0.00 |n_ep    50 |ep_len   21.0 |ep_rew  20.95 |raw_ep_rew  20.95 |env_step   1200 |time 00:10 rem 02:46\n",
      "iter    800 |loss   0.01 |n_ep    72 |ep_len   18.2 |ep_rew  18.23 |raw_ep_rew  18.23 |env_step   1600 |time 00:15 rem 02:53\n",
      "iter   1000 |loss   0.01 |n_ep    92 |ep_len   18.5 |ep_rew  18.54 |raw_ep_rew  18.54 |env_step   2000 |time 00:19 rem 02:52\n",
      "iter   1200 |loss   0.01 |n_ep   109 |ep_len   19.8 |ep_rew  19.85 |raw_ep_rew  19.85 |env_step   2400 |time 00:23 rem 02:49\n",
      "iter   1400 |loss   0.03 |n_ep   122 |ep_len   27.1 |ep_rew  27.08 |raw_ep_rew  27.08 |env_step   2800 |time 00:27 rem 02:46\n",
      "iter   1600 |loss   0.02 |n_ep   139 |ep_len   23.8 |ep_rew  23.83 |raw_ep_rew  23.83 |env_step   3200 |time 00:31 rem 02:46\n",
      "iter   1800 |loss   0.01 |n_ep   160 |ep_len   17.4 |ep_rew  17.39 |raw_ep_rew  17.39 |env_step   3600 |time 00:35 rem 02:43\n",
      "iter   2000 |loss   0.05 |n_ep   181 |ep_len   17.9 |ep_rew  17.91 |raw_ep_rew  17.91 |env_step   4000 |time 00:40 rem 02:41\n",
      "iter   2200 |loss   0.04 |n_ep   197 |ep_len   21.5 |ep_rew  21.52 |raw_ep_rew  21.52 |env_step   4400 |time 00:44 rem 02:38\n",
      "iter   2400 |loss   0.11 |n_ep   213 |ep_len   24.3 |ep_rew  24.35 |raw_ep_rew  24.35 |env_step   4800 |time 00:48 rem 02:34\n",
      "iter   2600 |loss   0.05 |n_ep   223 |ep_len   34.1 |ep_rew  34.12 |raw_ep_rew  34.12 |env_step   5200 |time 00:52 rem 02:30\n",
      "iter   2800 |loss   0.07 |n_ep   233 |ep_len   36.0 |ep_rew  36.03 |raw_ep_rew  36.03 |env_step   5600 |time 00:57 rem 02:26\n",
      "iter   3000 |loss   0.08 |n_ep   243 |ep_len   30.6 |ep_rew  30.59 |raw_ep_rew  30.59 |env_step   6000 |time 01:01 rem 02:23\n",
      "iter   3200 |loss   0.11 |n_ep   257 |ep_len   28.0 |ep_rew  28.05 |raw_ep_rew  28.05 |env_step   6400 |time 01:06 rem 02:20\n",
      "iter   3400 |loss   0.06 |n_ep   261 |ep_len   46.4 |ep_rew  46.43 |raw_ep_rew  46.43 |env_step   6800 |time 01:10 rem 02:16\n",
      "iter   3600 |loss   0.08 |n_ep   263 |ep_len   64.0 |ep_rew  63.98 |raw_ep_rew  63.98 |env_step   7200 |time 01:14 rem 02:11\n",
      "iter   3800 |loss   0.05 |n_ep   269 |ep_len   69.7 |ep_rew  69.70 |raw_ep_rew  69.70 |env_step   7600 |time 01:18 rem 02:07\n",
      "iter   4000 |loss   0.10 |n_ep   271 |ep_len   95.5 |ep_rew  95.46 |raw_ep_rew  95.46 |env_step   8000 |time 01:22 rem 02:03\n",
      "iter   4200 |loss   0.12 |n_ep   273 |ep_len  124.4 |ep_rew 124.40 |raw_ep_rew 124.40 |env_step   8400 |time 01:26 rem 01:58\n",
      "iter   4400 |loss   0.19 |n_ep   275 |ep_len  131.9 |ep_rew 131.94 |raw_ep_rew 131.94 |env_step   8800 |time 01:30 rem 01:54\n",
      "iter   4600 |loss   0.18 |n_ep   276 |ep_len  135.9 |ep_rew 135.95 |raw_ep_rew 135.95 |env_step   9200 |time 01:34 rem 01:50\n",
      "iter   4800 |loss   0.23 |n_ep   279 |ep_len  161.1 |ep_rew 161.10 |raw_ep_rew 161.10 |env_step   9600 |time 01:38 rem 01:46\n",
      "iter   5000 |loss   0.11 |n_ep   282 |ep_len  154.1 |ep_rew 154.10 |raw_ep_rew 154.10 |env_step  10000 |time 01:42 rem 01:42\n",
      "iter   5200 |loss   0.38 |n_ep   284 |ep_len  158.3 |ep_rew 158.29 |raw_ep_rew 158.29 |env_step  10400 |time 01:46 rem 01:38\n",
      "iter   5400 |loss   0.02 |n_ep   286 |ep_len  154.7 |ep_rew 154.70 |raw_ep_rew 154.70 |env_step  10800 |time 01:50 rem 01:33\n",
      "iter   5600 |loss   0.02 |n_ep   288 |ep_len  154.5 |ep_rew 154.54 |raw_ep_rew 154.54 |env_step  11200 |time 01:54 rem 01:29\n",
      "iter   5800 |loss   0.28 |n_ep   290 |ep_len  174.3 |ep_rew 174.32 |raw_ep_rew 174.32 |env_step  11600 |time 01:58 rem 01:25\n",
      "iter   6000 |loss   0.20 |n_ep   292 |ep_len  172.0 |ep_rew 172.02 |raw_ep_rew 172.02 |env_step  12000 |time 02:02 rem 01:21\n",
      "iter   6200 |loss   0.16 |n_ep   295 |ep_len  185.3 |ep_rew 185.32 |raw_ep_rew 185.32 |env_step  12400 |time 02:06 rem 01:17\n",
      "iter   6400 |loss   0.23 |n_ep   296 |ep_len  183.4 |ep_rew 183.39 |raw_ep_rew 183.39 |env_step  12800 |time 02:10 rem 01:13\n",
      "iter   6600 |loss   0.05 |n_ep   299 |ep_len  182.3 |ep_rew 182.31 |raw_ep_rew 182.31 |env_step  13200 |time 02:14 rem 01:09\n",
      "iter   6800 |loss   0.34 |n_ep   301 |ep_len  194.7 |ep_rew 194.67 |raw_ep_rew 194.67 |env_step  13600 |time 02:18 rem 01:05\n",
      "iter   7000 |loss   0.04 |n_ep   303 |ep_len  195.2 |ep_rew 195.22 |raw_ep_rew 195.22 |env_step  14000 |time 02:22 rem 01:00\n",
      "iter   7200 |loss   0.07 |n_ep   305 |ep_len  194.9 |ep_rew 194.94 |raw_ep_rew 194.94 |env_step  14400 |time 02:26 rem 00:56\n",
      "iter   7400 |loss   0.05 |n_ep   307 |ep_len  195.4 |ep_rew 195.41 |raw_ep_rew 195.41 |env_step  14800 |time 02:30 rem 00:52\n",
      "iter   7600 |loss   0.37 |n_ep   308 |ep_len  197.0 |ep_rew 196.97 |raw_ep_rew 196.97 |env_step  15200 |time 02:34 rem 00:48\n",
      "iter   7800 |loss   0.04 |n_ep   309 |ep_len  202.4 |ep_rew 202.37 |raw_ep_rew 202.37 |env_step  15600 |time 02:38 rem 00:44\n",
      "iter   8000 |loss   0.05 |n_ep   311 |ep_len  220.6 |ep_rew 220.59 |raw_ep_rew 220.59 |env_step  16000 |time 02:42 rem 00:40\n",
      "iter   8200 |loss   0.08 |n_ep   313 |ep_len  215.9 |ep_rew 215.95 |raw_ep_rew 215.95 |env_step  16400 |time 02:46 rem 00:36\n",
      "iter   8400 |loss   1.86 |n_ep   315 |ep_len  227.8 |ep_rew 227.81 |raw_ep_rew 227.81 |env_step  16800 |time 02:50 rem 00:32\n",
      "iter   8600 |loss   0.09 |n_ep   315 |ep_len  227.8 |ep_rew 227.81 |raw_ep_rew 227.81 |env_step  17200 |time 02:54 rem 00:28\n",
      "iter   8800 |loss   0.02 |n_ep   317 |ep_len  236.4 |ep_rew 236.42 |raw_ep_rew 236.42 |env_step  17600 |time 02:58 rem 00:24\n",
      "iter   9000 |loss   0.39 |n_ep   319 |ep_len  234.5 |ep_rew 234.48 |raw_ep_rew 234.48 |env_step  18000 |time 03:02 rem 00:20\n",
      "iter   9200 |loss   0.05 |n_ep   321 |ep_len  239.9 |ep_rew 239.87 |raw_ep_rew 239.87 |env_step  18400 |time 03:06 rem 00:16\n",
      "iter   9400 |loss   0.05 |n_ep   322 |ep_len  248.6 |ep_rew 248.58 |raw_ep_rew 248.58 |env_step  18800 |time 03:10 rem 00:12\n",
      "iter   9600 |loss   0.03 |n_ep   324 |ep_len  245.7 |ep_rew 245.65 |raw_ep_rew 245.65 |env_step  19200 |time 03:14 rem 00:08\n",
      "iter   9800 |loss   0.61 |n_ep   325 |ep_len  248.0 |ep_rew 247.99 |raw_ep_rew 247.99 |env_step  19600 |time 03:18 rem 00:04\n",
      "save checkpoint to cartpole_dqn/9999.pth\n"
     ]
    }
   ],
   "source": [
    "%run Main.py  \\\n",
    "    --niter 10000   \\\n",
    "    --env CartPole-v1   \\\n",
    "    --algo dqn  \\\n",
    "    --nproc 2   \\\n",
    "    --lr 0.001  \\\n",
    "    --train_freq 1  \\\n",
    "    --train_start 100   \\\n",
    "    --replay_size 20000 \\\n",
    "    --batch_size 64     \\\n",
    "    --discount 0.996    \\\n",
    "    --target_update 1000    \\\n",
    "    --eps_decay 4000    \\\n",
    "    --print_freq 200    \\\n",
    "    --checkpoint_freq 20000 \\\n",
    "    --save_dir cartpole_dqn \\\n",
    "    --log log.txt \\\n",
    "    --parallel_env 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curve(logfile, title=None):\n",
    "    lines = open(logfile, 'r').readlines()\n",
    "    lines = [l.split() for l in lines if l[:4] == 'iter']\n",
    "    steps = [int(l[13]) for l in lines]\n",
    "    rewards = [float(l[11]) for l in lines]\n",
    "    plt.plot(steps, rewards)\n",
    "    plt.xlabel('env steps'); plt.ylabel('avg episode reward'); plt.grid(True)\n",
    "    if title: plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log is saved to `'cartpole_dqn/log.txt'`. Let's plot the running averaged episode reward curve during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cartpole_dqn/log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f1625ee004a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cartpole_dqn/log.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CartPole DQN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-272a5cc34516>\u001b[0m in \u001b[0;36mplot_curve\u001b[1;34m(logfile, title)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'iter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cartpole_dqn/log.txt'"
     ]
    }
   ],
   "source": [
    "plot_curve('cartpole_dqn/log.txt', 'CartPole DQN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Actor-Critic Algorithm\n",
    "\n",
    "Policy gradient methods are another class of algorithms that originated from viewing the RL problem as a mathematical optimization problem. Recall that the objective of RL is to maximize the expected cumulative reward the agent gets, namely\n",
    "$$\n",
    "\\max_{\\pi} J(\\pi) := \\mathbb{E}_{ (s_t,a_t,r_t)\\sim D^{\\pi} } \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n",
    "$$\n",
    "where $D^{\\pi}$ is the distribution of trajectories induced by policy $\\pi$, and inside the expectation is the random variable representing the discounted cumulative reward and $J$ is the reward (or cost) functional. Essentially, we want to optimize the policy $\\pi$.\n",
    "\n",
    "The most straightforward way is to run gradient update on the parameter $\\theta$ of a *parameterized* policy $\\pi_{\\theta}$. One such algorithm is the so-called `Advantage Actor-Critic (A2C)`. A2C is an on-policy policy optimization type algorithm. While collecting on-policy data, we iteratively run gradient ascent:\n",
    "$$\n",
    "\\theta_{new} \\leftarrow \\theta_{old} + \\eta { \\hat \\nabla_{\\theta} } J(\\pi_{\\theta_{old}})\n",
    "$$\n",
    "with a Monte Carlo estimate ${ \\hat \\nabla_{\\theta} } J$ of the true gradient $\\nabla_{\\theta} J$. The true gradient writes as (by Policy Gradient Theorem and some manipulations):\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta_{old}}) = \\mathbb{E}_{ (s_t,a_t,r_t)\\sim D^{ \\pi_{\\theta_{old}} } } \\sum_{t=0}^{\\infty} \\left( \\nabla_{\\theta} \\log \\pi_{\\theta_{old}} (s_t, a_t) \\left( \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} r_{t'} - V^{ \\pi_{\\theta_{old}} }(s_t) \\right) \\right)  .\n",
    "$$\n",
    "The quantity in the inner-most parentheses $A(s_t, a_t) = Q(s_t, a_t) - V(s_t) = (\\mathbb{E} \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} r_{t'}) - V(s_t)$ is called the *Advantage* function (not very rigoriously speaking...). That's why it's called **Advantage** Actor-Critic. More on A2C: https://arxiv.org/abs/1506.02438.\n",
    "\n",
    "And the Monte Carlo estimate of the gradient is\n",
    "$$\n",
    "{ \\hat \\nabla_{\\theta} } J(\\pi_{\\theta_{old}}) = \\frac1{NT}  \\sum_{i=1}^N \\sum_{t=0}^T \\left( \\nabla_{\\theta} \\log \\pi_{\\theta_{old}} (s_t^{i}, a_t^{i}) \\left( \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}^{i} - V_{\\phi_{old}}(s_t^{i}) \\right) \\right)\n",
    "$$\n",
    "where $V_{\\phi_{old}}$ is introduced as a *parameterized* estimate for $V^{ \\pi_{\\theta_{old}} }$, which can also be a neural network. So $V_{\\phi}$ is the **critic** and $\\pi_{\\theta}$ is the **actor**. We can construct a specific loss function in pytorch that gives ${ \\hat \\nabla_{\\theta} } J$. $V_{\\phi_{old}}$ is trained with SGD on a L2 loss function. It's further common practice to add an entropy bonus loss term to encourage maximum entropy solution, to facilitate exploration and avoid getting stuck in local minima. We shall clarify these loss functions in the following summarization.\n",
    "\n",
    "#### Summarizing a variant of the A2C algorithm:\n",
    "> For many iterations repeat:\n",
    "1. Collect $N$ independent trajectories $\\{ (s_t^{i},a_t^{i},r_t^{i})_{t=0}^T \\}_{i=1}^{N}$ by running policy $\\pi_{\\theta}$ for maximum $T$ steps;\n",
    "2. Compute the loss function for the policy parameter $\\theta$:\n",
    "$$\n",
    "L_{policy}(\\theta) = \\frac1{NT} \\sum_{i=1}^N \\sum_{t=0}^T \\left( \\log \\pi_{\\theta} (s_t^{i}, a_t^{i}) \\left( \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}^{i} - V_{\\phi}(s_t^{i}) \\right) \\right)\n",
    "$$\n",
    "Compute the entropy term for $\\theta$:\n",
    "$$\n",
    "L_{entropy}(\\theta) = \\frac1{NT} \\sum_{i=1}^N \\sum_{t=0}^T \\left( - \\sum_{a\\in A} \\pi_{\\theta}(s_t^{i}, a) \\log \\pi_{\\theta}(s_t^{i}, a) \\right)\n",
    "$$\n",
    "Compute the loss for value function parameter $\\phi$:\n",
    "$$\n",
    "L_{value}(\\phi) = \\frac1{NT} \\sum_{i=1}^N \\sum_{t=0}^T \\left( \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}^{i} - V_{\\phi}(s_t^{i}) \\right)^2\n",
    "$$\n",
    "3. Use pytorch auto-differentiation and optimizer to do one gradient step on $(\\theta, \\phi)$ with the overall loss:\n",
    "$$\n",
    "L(\\theta, \\phi) = - L_{policy} - \\lambda_{ent} L_{entropy} + \\lambda_{val} L_{value}\n",
    "$$\n",
    "where $\\lambda_{ent}$ and $\\lambda_{val}$ are coefficients to balances the loss terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C4 (10 pts): Complete the code for computing the advantange, entropy and loss function in `A2C.train` in file `Algo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P2 (10 pts): Run A2C on CartPole and plot the learning curve (i.e. averaged episodic reward against training iteration).\n",
    "Your code should be able to achieve **>150** averaged reward in 10000 iterations (40000 simulation steps) in only a few minutes. This is a good indication that the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(algo='a2c', batch_size=64, checkpoint_freq=20000, discount=0.996, ent_coef=0.01, env='CartPole-v1', eps_decay=200000, frame_skip=1, frame_stack=4, load='', log='log.txt', lr=0.001, niter=10000, nproc=4, parallel_env=0, print_freq=200, replay_size=1000000, save_dir='cartpole_a2c/', target_update=2500, train_freq=16, train_start=0, value_coef=0.01)\n",
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "running on device cuda\n",
      "shared net = False, parameters to optimize: [('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([2, 128]), True), ('fc2.bias', torch.Size([2]), True), ('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([1, 128]), True), ('fc2.bias', torch.Size([1]), True)] \n",
      "\n",
      "obses on reset: 4 x (4,) float32\n",
      "iter    200 |loss   0.94 |n_ep    30 |ep_len   27.8 |ep_rew  27.84 |raw_ep_rew  27.84 |env_step    800 |time 00:00 rem 00:23\n",
      "iter    400 |loss   0.81 |n_ep    63 |ep_len   29.2 |ep_rew  29.17 |raw_ep_rew  29.17 |env_step   1600 |time 00:00 rem 00:23\n",
      "iter    600 |loss   0.77 |n_ep    97 |ep_len   24.9 |ep_rew  24.85 |raw_ep_rew  24.85 |env_step   2400 |time 00:01 rem 00:22\n",
      "iter    800 |loss   0.76 |n_ep   129 |ep_len   25.6 |ep_rew  25.62 |raw_ep_rew  25.62 |env_step   3200 |time 00:01 rem 00:22\n",
      "iter   1000 |loss   0.66 |n_ep   153 |ep_len   27.9 |ep_rew  27.95 |raw_ep_rew  27.95 |env_step   4000 |time 00:02 rem 00:21\n",
      "iter   1200 |loss   0.79 |n_ep   175 |ep_len   35.9 |ep_rew  35.93 |raw_ep_rew  35.93 |env_step   4800 |time 00:02 rem 00:21\n",
      "iter   1400 |loss   0.98 |n_ep   194 |ep_len   37.5 |ep_rew  37.52 |raw_ep_rew  37.52 |env_step   5600 |time 00:03 rem 00:21\n",
      "iter   1600 |loss   0.70 |n_ep   213 |ep_len   40.3 |ep_rew  40.26 |raw_ep_rew  40.26 |env_step   6400 |time 00:03 rem 00:20\n",
      "iter   1800 |loss   0.75 |n_ep   234 |ep_len   36.1 |ep_rew  36.08 |raw_ep_rew  36.08 |env_step   7200 |time 00:04 rem 00:20\n",
      "iter   2000 |loss   0.66 |n_ep   248 |ep_len   54.1 |ep_rew  54.10 |raw_ep_rew  54.10 |env_step   8000 |time 00:04 rem 00:19\n",
      "iter   2200 |loss   1.07 |n_ep   264 |ep_len   40.5 |ep_rew  40.45 |raw_ep_rew  40.45 |env_step   8800 |time 00:05 rem 00:19\n",
      "iter   2400 |loss   0.73 |n_ep   276 |ep_len   60.4 |ep_rew  60.41 |raw_ep_rew  60.41 |env_step   9600 |time 00:05 rem 00:18\n",
      "iter   2600 |loss   0.60 |n_ep   289 |ep_len   59.5 |ep_rew  59.50 |raw_ep_rew  59.50 |env_step  10400 |time 00:06 rem 00:18\n",
      "iter   2800 |loss   0.90 |n_ep   305 |ep_len   58.8 |ep_rew  58.78 |raw_ep_rew  58.78 |env_step  11200 |time 00:06 rem 00:17\n",
      "iter   3000 |loss   0.56 |n_ep   318 |ep_len   56.7 |ep_rew  56.71 |raw_ep_rew  56.71 |env_step  12000 |time 00:07 rem 00:16\n",
      "iter   3200 |loss   0.60 |n_ep   331 |ep_len   62.9 |ep_rew  62.90 |raw_ep_rew  62.90 |env_step  12800 |time 00:07 rem 00:16\n",
      "iter   3400 |loss   0.84 |n_ep   345 |ep_len   57.4 |ep_rew  57.40 |raw_ep_rew  57.40 |env_step  13600 |time 00:08 rem 00:15\n",
      "iter   3600 |loss   0.26 |n_ep   359 |ep_len   65.2 |ep_rew  65.21 |raw_ep_rew  65.21 |env_step  14400 |time 00:08 rem 00:15\n",
      "iter   3800 |loss   0.51 |n_ep   369 |ep_len   75.8 |ep_rew  75.80 |raw_ep_rew  75.80 |env_step  15200 |time 00:09 rem 00:14\n",
      "iter   4000 |loss   0.42 |n_ep   376 |ep_len   88.7 |ep_rew  88.75 |raw_ep_rew  88.75 |env_step  16000 |time 00:09 rem 00:14\n",
      "iter   4200 |loss   0.52 |n_ep   383 |ep_len   96.3 |ep_rew  96.29 |raw_ep_rew  96.29 |env_step  16800 |time 00:10 rem 00:13\n",
      "iter   4400 |loss   0.17 |n_ep   392 |ep_len   98.0 |ep_rew  98.03 |raw_ep_rew  98.03 |env_step  17600 |time 00:10 rem 00:13\n",
      "iter   4600 |loss   0.97 |n_ep   399 |ep_len   99.3 |ep_rew  99.34 |raw_ep_rew  99.34 |env_step  18400 |time 00:11 rem 00:12\n",
      "iter   4800 |loss   0.25 |n_ep   409 |ep_len   73.9 |ep_rew  73.88 |raw_ep_rew  73.88 |env_step  19200 |time 00:11 rem 00:12\n",
      "iter   5000 |loss   0.86 |n_ep   415 |ep_len   98.3 |ep_rew  98.27 |raw_ep_rew  98.27 |env_step  20000 |time 00:11 rem 00:11\n",
      "iter   5200 |loss   0.19 |n_ep   419 |ep_len  139.9 |ep_rew 139.94 |raw_ep_rew 139.94 |env_step  20800 |time 00:12 rem 00:11\n",
      "iter   5400 |loss   0.32 |n_ep   424 |ep_len  150.0 |ep_rew 150.03 |raw_ep_rew 150.03 |env_step  21600 |time 00:12 rem 00:10\n",
      "iter   5600 |loss   0.90 |n_ep   430 |ep_len  142.5 |ep_rew 142.52 |raw_ep_rew 142.52 |env_step  22400 |time 00:13 rem 00:10\n",
      "iter   5800 |loss   0.51 |n_ep   435 |ep_len  153.2 |ep_rew 153.23 |raw_ep_rew 153.23 |env_step  23200 |time 00:13 rem 00:09\n",
      "iter   6000 |loss   0.22 |n_ep   440 |ep_len  143.6 |ep_rew 143.60 |raw_ep_rew 143.60 |env_step  24000 |time 00:14 rem 00:09\n",
      "iter   6200 |loss   0.93 |n_ep   443 |ep_len  149.6 |ep_rew 149.64 |raw_ep_rew 149.64 |env_step  24800 |time 00:14 rem 00:09\n",
      "iter   6400 |loss   0.14 |n_ep   450 |ep_len  165.7 |ep_rew 165.67 |raw_ep_rew 165.67 |env_step  25600 |time 00:15 rem 00:08\n",
      "iter   6600 |loss   0.23 |n_ep   457 |ep_len  143.8 |ep_rew 143.77 |raw_ep_rew 143.77 |env_step  26400 |time 00:15 rem 00:08\n",
      "iter   6800 |loss   0.73 |n_ep   461 |ep_len  132.6 |ep_rew 132.62 |raw_ep_rew 132.62 |env_step  27200 |time 00:16 rem 00:07\n",
      "iter   7000 |loss   0.99 |n_ep   464 |ep_len  130.0 |ep_rew 129.98 |raw_ep_rew 129.98 |env_step  28000 |time 00:16 rem 00:07\n",
      "iter   7200 |loss   0.86 |n_ep   468 |ep_len  170.1 |ep_rew 170.15 |raw_ep_rew 170.15 |env_step  28800 |time 00:17 rem 00:06\n",
      "iter   7400 |loss   0.04 |n_ep   474 |ep_len  175.0 |ep_rew 174.99 |raw_ep_rew 174.99 |env_step  29600 |time 00:17 rem 00:06\n",
      "iter   7600 |loss  -0.20 |n_ep   479 |ep_len  166.0 |ep_rew 165.96 |raw_ep_rew 165.96 |env_step  30400 |time 00:18 rem 00:05\n",
      "iter   7800 |loss   0.90 |n_ep   482 |ep_len  184.9 |ep_rew 184.95 |raw_ep_rew 184.95 |env_step  31200 |time 00:18 rem 00:05\n",
      "iter   8000 |loss   0.72 |n_ep   486 |ep_len  182.2 |ep_rew 182.18 |raw_ep_rew 182.18 |env_step  32000 |time 00:19 rem 00:04\n",
      "iter   8200 |loss  -0.14 |n_ep   490 |ep_len  204.9 |ep_rew 204.94 |raw_ep_rew 204.94 |env_step  32800 |time 00:19 rem 00:04\n",
      "iter   8400 |loss   0.95 |n_ep   494 |ep_len  190.4 |ep_rew 190.39 |raw_ep_rew 190.39 |env_step  33600 |time 00:19 rem 00:03\n",
      "iter   8600 |loss  -0.04 |n_ep   499 |ep_len  210.5 |ep_rew 210.51 |raw_ep_rew 210.51 |env_step  34400 |time 00:20 rem 00:03\n",
      "iter   8800 |loss   0.84 |n_ep   503 |ep_len  188.5 |ep_rew 188.54 |raw_ep_rew 188.54 |env_step  35200 |time 00:20 rem 00:02\n",
      "iter   9000 |loss   0.79 |n_ep   508 |ep_len  194.9 |ep_rew 194.87 |raw_ep_rew 194.87 |env_step  36000 |time 00:21 rem 00:02\n",
      "iter   9200 |loss   0.06 |n_ep   511 |ep_len  205.6 |ep_rew 205.64 |raw_ep_rew 205.64 |env_step  36800 |time 00:21 rem 00:01\n",
      "iter   9400 |loss   0.65 |n_ep   513 |ep_len  216.9 |ep_rew 216.93 |raw_ep_rew 216.93 |env_step  37600 |time 00:22 rem 00:01\n",
      "iter   9600 |loss   0.20 |n_ep   518 |ep_len  218.7 |ep_rew 218.65 |raw_ep_rew 218.65 |env_step  38400 |time 00:22 rem 00:00\n",
      "iter   9800 |loss   0.39 |n_ep   521 |ep_len  214.7 |ep_rew 214.71 |raw_ep_rew 214.71 |env_step  39200 |time 00:23 rem 00:00\n",
      "save checkpoint to cartpole_a2c/9999.pth\n"
     ]
    }
   ],
   "source": [
    "%run Main.py  \\\n",
    "    --niter 10000   \\\n",
    "    --env CartPole-v1   \\\n",
    "    --algo a2c  \\\n",
    "    --nproc 4   \\\n",
    "    --lr 0.001  \\\n",
    "    --train_freq 16 \\\n",
    "    --train_start 0 \\\n",
    "    --batch_size 64     \\\n",
    "    --discount 0.996    \\\n",
    "    --value_coef 0.01    \\\n",
    "    --print_freq 200    \\\n",
    "    --checkpoint_freq 20000 \\\n",
    "    --save_dir cartpole_a2c \\\n",
    "    --log log.txt \\\n",
    "    --parallel_env 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curve('cartpole_a2c/log.txt', 'CartPole A2C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play a little bit with the trained agent. The neural net parameters are saved to the `cartpole_dqn` and `cartpole_a2c` folders. The cell below will open a window showing one episode play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared net = False, parameters to optimize: [('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([2, 128]), True), ('fc2.bias', torch.Size([2]), True), ('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([1, 128]), True), ('fc2.bias', torch.Size([1]), True)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "import Algo\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = Algo.ActorCritic(env.observation_space, env.action_space)\n",
    "agent.load('cartpole_a2c/9999.pth')\n",
    "state = env.reset()\n",
    "for _ in range(120):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(agent.act([state])[0])\n",
    "    if done: break\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Solve the Atari Breakout game\n",
    "***\n",
    "In this part, you'll train your agent to play Breakout with the BlueWaters cluster. I have provided the job scripts for you. Please upload your `Algo.py` and `Model.py` completed in **Part I** to your BlueWaters folder. And submit the following two jobs respectively:\n",
    "```\n",
    "qsub run_dqn.pbs\n",
    "qsub run_a2c.pbs\n",
    "```\n",
    "\n",
    "The jobs are set to run for at most **14 hours**. **<font color=red>Please start early!!</font>** You might be able to reach the desired score (>= 200 reward) before 14 hours - You can stop the training early if you wish. Then please collect the resulting `breakout_dqn/log.txt` and `breakout_a2c/log.txt` files into the same folder as this Jupyter notebook's. Rename them as `log_breakout_dqn.txt` and `log_breakout_a2c.txt`.\n",
    "\n",
    "BTW, there's an Atari PC simulator: https://stella-emu.github.io/ I spent a lot of time playing them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C5 (10 pts): Complete the code for the CNN with 3 conv layers and 3 fc layers in class `SimpleCNN` in file `Model.py`\n",
    "And verify the output shape with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo\n",
      "32\n",
      "8\n",
      "4\n",
      "Helllo\n",
      "64\n",
      "4\n",
      "2\n",
      "Helllo\n",
      "64\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "n_in=[4,84,84]\n",
    "conv_channels=[32, 64, 64]\n",
    "conv_kernels=[8, 4, 3] \n",
    "conv_strides=[4, 2, 1] \n",
    "n_fc=[256] \n",
    "n_out=4\n",
    "conv_layers = []\n",
    "c0 = n_in[0]\n",
    "h0 = n_in[1]\n",
    "for c, k, s in zip(conv_channels, conv_kernels, conv_strides):\n",
    "    print(\"Helllo\")\n",
    "    print(c)\n",
    "    print(k)\n",
    "    print(s)\n",
    "    conv_layers.append(nn.Conv2d(c0, c, kernel_size=k, stride=s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-54166fdc4151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mModel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m84\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\UIUC\\Classes\\IE_534\\IE_534\\HW8\\Model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_in, conv_channels, conv_kernels, conv_strides, n_fc, n_out)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;31m# append nn.Conv2d with kernel size k, stride s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# self.conv_layers.append(nn.Conv2d(c0, c, kernel_size=k, stride=s))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\u001b[0m\n\u001b[0;32m    328\u001b[0m         super(Conv2d, self).__init__(\n\u001b[0;32m    329\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             False, _pair(0), groups, bias, padding_mode)\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode)\u001b[0m\n\u001b[0;32m     20\u001b[0m                  groups, bias, padding_mode):\n\u001b[0;32m     21\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0min_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'in_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for %: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "## Test code\n",
    "from Model import SimpleCNN\n",
    "import torch\n",
    "net = SimpleCNN()\n",
    "x = torch.randn(2, 4, 84, 84)\n",
    "y = net(x)\n",
    "assert y.shape == (2, 4), \"ERROR: network output has the wrong shape!\"\n",
    "print (\"CNN output shape test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P3 (10 pts): Run the following cell to generate a DQN learning curve.\n",
    "The *maximum* average episodic reward on this curve should be larger than $200$ for full credit. (It's ok if the final reward is not as high.) The typical value is around $300$. You get 70% credit if $100 \\le$ average episodic reward $< 200$, 50% credit if $50 \\le$ average episodic reward $< 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curve('log_breakout_dqn.txt', 'Breakout DQN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P4 (10 pts): Run the following cell to generate an A2C learning curve.\n",
    "The *maximum* average episodic reward on this curve should be larger than $150$ for full credit. (It's ok if the final reward is not as high.) The typical value is around $250$. You get 70% credit if $50 \\le$ average episodic reward $< 150$, and 50% credit if $20 \\le$ average episodic reward $< 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve('log_breakout_a2c.txt', 'Breakout A2C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P5 (10 pts): Collect and visualize some game frames by running the script `Draw.py` on BlueWaters.\n",
    "(1) `module load python/2.0.0` and run `Draw.py` on BlueWaters (it's ok to run this locally, no need to start a job).\n",
    "\n",
    "(2) Download the result `breakout_imgs` folder from BlueWaters to the folder containing this Jupyter notebook, and run the following cell. You should see some animation of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "imgs = sorted(os.listdir('breakout_imgs'))\n",
    "#imgs = [plt.imread('breakout_imgs/' + img) for img in imgs]\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "pimg = None\n",
    "for img in imgs:\n",
    "    img = plt.imread('breakout_imgs/' + img)\n",
    "    if pimg:\n",
    "        pimg.set_data(img)\n",
    "    else:\n",
    "        pimg = plt.imshow(img)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Questions (10 pts)\n",
    "***\n",
    "\n",
    "These are open-ended questions. The purpose is to encourage you to think (a bit) more deeply about these algorithms. You get full points as long as you write a few sentences that make sense and show some thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1 (2 pts): Why would people want to do function approximation rather than using tabular algorithm (on discretized S,A spaces if necessary)? Bringing function approximation has caused numerous problems theoretically (e.g. not guaranteed to converge), so it seems not worth it..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. People enjoy \"neuralizing\" things I guess.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2 (2 pts): Q-Learning seems good... it's theoretically sound (at least seems to be), the performance is also good. Why would many people actually prefer policy gradient type algorithms in some practical problems?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. I like Q learning. The name is cute. Anyone watch StarTrek?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3 (2 pts): Does the policy gradient algorithm (A2C) we implemented here extend to continuous action space? How would you do that? Hint: What is a reasonable distribution assumption for policy $\\pi_{\\theta}(a|s)$ if $a$ lives in continuous space?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. Maybe normalizing flow?? OK, people really do this..(arXiv:1905.06893) Hot area + hot area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4 (2 pts): The policy gradient algorithm (A2C) we implemented uses on-policy data. Can you think of a way to extend it to utilize off-policy data? Hint: Importance sampling, needs some approximation though"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. Do random math tricks or pray?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5 (2 pts): How to compare different RL algorithms? When can I say one algorithm is better than the other? Hint: This question is quite open. Think about speed, complexity, tasks, etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. Just pick one you like, they're equally bad.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
